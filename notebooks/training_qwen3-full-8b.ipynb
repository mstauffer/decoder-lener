{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0922d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.qwen3 import Qwen3\n",
    "\n",
    "t = Qwen3(model_name = \"Qwen/Qwen3-8B\", device = \"cuda:7\")\n",
    "\n",
    "t.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.lener import LenerDataset\n",
    "\n",
    "l = LenerDataset(tokenizer=t.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c622cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = l.load_dataset()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48ab04-e886-40ce-902a-14889451f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all = trainable params: 21,823,488 || all params: 8,212,558,848 || trainable%: 0.2657\n",
    "# full_att = trainable params: 7,667,712 || all params: 8,198,403,072 || trainable%: 0.0935\n",
    "# ffn = trainable params: 9,437,184 || all params: 8,200,172,544 || trainable%: 0.1151\n",
    "## full_att_plus_ffn: trainable params: 17,104,896 || all params: 8,207,840,256 || trainable%: 0.2084\n",
    "# low_rank = trainable params: 10,911,744 || all params: 8,201,647,104 || trainable%: 0.1330\n",
    "# high_rank = trainable params: 87,293,952 || all params: 8,278,029,312 || trainable%: 1.0545\n",
    "# high XX = trainable params: 349,175,808 || all params: 8,539,911,168 || trainable%: 4.0888\n",
    "# baseline = trainable params: 1,179,648 || all params: 8,191,915,008 || trainable%: 0.0144\n",
    "#  0.6B_all = trainable params: 5,046,272 || all params: 601,096,192 || trainable%: 0.8395\n",
    "# 1.7B = trainable params: 8,716,288 || all params: 1,729,291,264 || trainable%: 0.5040\n",
    "# 4B = trainable params: 16,515,072 || all params: 4,038,983,168 || trainable%: 0.4089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.peft_configs.lora import LoraAdapter\n",
    "# PRESET = 'baseline'\n",
    "# PRESET = 'full_attention'\n",
    "PRESET = 'all'\n",
    "# PRESET = 'ffn'\n",
    "# PRESET = 'full_attention_plus_ffn'\n",
    "# PRESET = 'low_rank'\n",
    "# PRESET = 'high_rank'\n",
    "# PRESET = 'high_rank_XX'\n",
    "\n",
    "lora = LoraAdapter(\n",
    "    model=t.model,\n",
    "    lora_preset = PRESET\n",
    ")\n",
    "\n",
    "model = lora.apply_lora()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71321c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./outputs/checkpoints/Qwen3_8b_full_{PRESET}',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    # device=model.device\n",
    "    \n",
    ")\n",
    "# model.to('cuda:7')\n",
    "# torch.cuda.set_device(7)\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    tokenizer=t.tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(t.tokenizer, mlm=False),\n",
    "    \n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ba376-0dd3-45cb-bd45-a4d6578cd571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
